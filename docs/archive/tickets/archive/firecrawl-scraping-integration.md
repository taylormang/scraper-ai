# Firecrawl scraping integration

## Description 

We need to solve for the following problems:
- IP banning, bot detection, proxy management
- Overall scraping complexity 

Our product's core work should be able to focus on the value: the conversation with your dataset 
Aha moment: getting the user to an aha moment (first conversation)
- Seamless prompt-based setup and extraction targeting, including for page elements like selectors
- Letting user talk to a database of their content

Unique positioning aspects should be focused on via our own logic and code for maximum control, but stuff like scraping logistics we want to offload 

## Vendor Overview
https://www.firecrawl.dev/ 

Firecrawl major feature options:
https://docs.firecrawl.dev/features/scrape
http://docs.firecrawl.dev/features/crawl
https://docs.firecrawl.dev/features/search
https://docs.firecrawl.dev/features/map

MCP Support
https://docs.firecrawl.dev/mcp-server

Advanced scraping guide
https://docs.firecrawl.dev/advanced-scraping-guide

## Strategic Analysis

### Current System Strengths
- **AI-Powered Extraction**: Custom OpenAI integration with template-based prompts
- **Schema Validation**: Auto-generated schemas with type enforcement
- **Navigation Caching**: 80-90% performance improvement for repeated patterns
- **Rate Limiting**: Centralized request management
- **Zero Configuration**: Works without user setup

### Firecrawl Advantages
- **Infrastructure**: Handles IP rotation, bot detection, proxy management
- **Reliability**: Stealth mode for advanced anti-bot protection
- **JavaScript Handling**: Native support for SPA and dynamic content
- **Maintenance**: Offloads scraping logistics to dedicated service
- **Scaling**: Built-in rate limiting and caching mechanisms

### Integration Strategy: Firecrawl-Only Approach

**Primary Strategy: Firecrawl as HTML Provider** (Required)
- Use Firecrawl for all HTML extraction and web scraping
- Keep our AI extraction, schema validation, and navigation caching
- Eliminate IP banning risks and infrastructure complexity
- Focus on our core value: AI-powered data processing and analysis

**Alternative Considered: Full Firecrawl Extraction**
- Use Firecrawl's LLM-based JSON extraction
- Replace our OpenAI integration entirely
- **Rejected**: Less control over extraction quality and consistency, loses our competitive advantages

## Technical Implementation Plan

### Phase 1: Firecrawl Service Integration

#### 1.1 Create Firecrawl Service (`firecrawl-service.ts`)
```typescript
export class FirecrawlService {
  private apiKey: string;
  private baseUrl: string = 'https://api.firecrawl.dev';

  // Single page scraping
  async scrapePage(url: string, options?: FirecrawlScrapeOptions): Promise<FirecrawlResult>

  // Multi-page crawling
  async crawlSite(url: string, options?: FirecrawlCrawlOptions): Promise<FirecrawlCrawlResult>

  // Check crawl status
  async getCrawlStatus(jobId: string): Promise<FirecrawlJobStatus>
}
```

#### 1.2 Define Firecrawl Types (`types.ts`)
```typescript
export interface FirecrawlScrapeOptions {
  formats?: ('markdown' | 'html' | 'rawHtml' | 'links' | 'screenshot')[];
  onlyMainContent?: boolean;
  includeTags?: string[];
  excludeTags?: string[];
  headers?: Record<string, string>;
  waitFor?: number;
  timeout?: number;
}

export interface FirecrawlResult {
  success: boolean;
  data?: {
    markdown?: string;
    html?: string;
    rawHtml?: string;
    metadata: {
      title: string;
      description: string;
      language: string;
      sourceURL: string;
    };
  };
  error?: string;
}
```

### Phase 2: Hybrid Scraper Implementation

#### 2.1 Update BasicScraper to use Firecrawl (`scraper.ts`)
```typescript
export class BasicScraper {
  private config: ScraperConfig;
  private aiExtractor: AIExtractor;
  private firecrawlService: FirecrawlService;
  private establishedSchema?: AutoGeneratedSchema;

  constructor(config: ScraperConfig) {
    this.config = config;
    this.aiExtractor = new AIExtractor();
    this.firecrawlService = new FirecrawlService(config.firecrawlApiKey);
  }

  async scrape(url: string): Promise<ScraperResult> {
    const startTime = Date.now();

    try {
      console.log(`üîç Starting Firecrawl scrape of: ${url}`);

      // Use Firecrawl for single-page or multi-page scraping
      if (this.config.maxPages && this.config.maxPages > 1) {
        return await this.multiPageScrapeWithFirecrawl(url);
      } else {
        return await this.singlePageScrapeWithFirecrawl(url);
      }

    } catch (error) {
      const duration = Date.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';

      console.log(`‚ùå Firecrawl scrape failed after ${duration}ms: ${errorMessage}`);

      return {
        success: false,
        error: errorMessage,
        duration
      };
    }
  }

  private async singlePageScrapeWithFirecrawl(url: string): Promise<ScraperResult> {
    // Use Firecrawl for reliable HTML extraction
    // Apply our AI extraction and schema validation
    // Return structured results
  }

  private async multiPageScrapeWithFirecrawl(url: string): Promise<ScraperResult> {
    // Use Firecrawl crawl feature for multi-page extraction
    // Apply our schema discovery and validation
    // Leverage our AI extraction on each page
  }
}
```

#### 2.2 Update ScraperConfig (`types.ts`)
```typescript
export interface ScraperConfig {
  // ... existing fields ...

  // Firecrawl integration (required)
  firecrawlApiKey: string;                   // Firecrawl API key (required)
  firecrawlOptions?: FirecrawlScrapeOptions; // Custom Firecrawl options

  // Legacy navigation fields (deprecated - Firecrawl handles navigation)
  navigationType?: 'button' | 'scroll' | 'none'; // Will be removed
  navigationPrompt?: string;                      // Will be removed
}
```

### Phase 3: Multi-Page Crawling Integration

#### 3.1 Enhance Firecrawl Service for Crawling
```typescript
export class FirecrawlService {
  // Multi-page crawling with our schema system
  async crawlWithSchema(url: string, extractionPrompt: string, options?: FirecrawlCrawlOptions): Promise<{
    success: boolean;
    data?: any[];
    schema?: AutoGeneratedSchema;
    jobId?: string;
    error?: string;
  }> {
    // Start Firecrawl crawl job
    const crawlJob = await this.startCrawl(url, {
      limit: options?.limit || 10,
      ...options
    });

    // Poll for completion
    const crawlResult = await this.waitForCrawlCompletion(crawlJob.jobId);

    // Apply our AI extraction to each page
    const extractedData = await this.extractDataFromPages(crawlResult.data, extractionPrompt);

    // Apply our schema discovery and validation
    const schema = SchemaDiscovery.discoverSchema(extractedData, configName);
    const validatedData = SchemaEnforcer.enforceSchema(extractedData, schema);

    return {
      success: true,
      data: validatedData.data,
      schema,
      jobId: crawlJob.jobId
    };
  }

  // Remove PageNavigator dependency - Firecrawl handles all navigation
  private async extractDataFromPages(pages: any[], extractionPrompt: string): Promise<any[]> {
    const allExtractedData: any[] = [];

    for (const page of pages) {
      // Use our AI extraction on Firecrawl-provided HTML
      const aiResult = await this.aiExtractor.extractStructuredData(page.html, extractionPrompt);

      if (aiResult.success && aiResult.data) {
        allExtractedData.push(...aiResult.data);
      }
    }

    return allExtractedData;
  }
}
```

### Phase 4: Configuration and Testing

#### 4.1 Update Test Configs
```typescript
export const testConfigs: Record<string, ScraperConfig> = {
  quotes: {
    name: 'quotes-scraper',
    baseUrl: 'http://quotes.toscrape.com',
    extractionPrompt: 'Extract each quote with: quote text, author name, and tags array',
    firecrawlApiKey: process.env.FIRECRAWL_API_KEY || '',
    maxPages: 3,
    firecrawlOptions: {
      formats: ['html'],
      onlyMainContent: true
    }
  },

  hackernews: {
    name: 'hackernews-scraper',
    baseUrl: 'https://news.ycombinator.com',
    extractionPrompt: 'Extract each article with: title, link URL, points, author, comment count',
    firecrawlApiKey: process.env.FIRECRAWL_API_KEY || '',
    maxPages: 3,
    firecrawlOptions: {
      formats: ['html'],
      waitFor: 2000,
      onlyMainContent: true
    }
  }
};
```

#### 4.2 Environment Configuration
```bash
# .env
OPENAI_API_KEY=your_openai_key
FIRECRAWL_API_KEY=your_firecrawl_key
```

### Phase 5: Advanced Features

#### 5.1 Cost Optimization
- **Caching Strategy**: Cache Firecrawl results to avoid redundant API calls
- **Usage Monitoring**: Track Firecrawl API usage and costs
- **Batch Processing**: Group multiple URLs for efficient crawling

#### 5.2 Performance Monitoring
```typescript
export interface ScrapingMetrics {
  method: 'firecrawl-scrape' | 'firecrawl-crawl';
  duration: number;
  success: boolean;
  pagesScraped: number;
  apiCallsCost: number;
  firecrawlJobId?: string;
  schemaDiscovered: boolean;
}
```

#### 5.3 Legacy Code Cleanup
- **Remove PageNavigator**: No longer needed - Firecrawl handles navigation
- **Remove Puppeteer Dependencies**: Significantly reduce bundle size
- **Remove Navigation Caching**: Firecrawl handles navigation efficiently
- **Simplify Configuration**: Remove navigation-related config options

### Benefits of Firecrawl-Only Approach

#### Immediate Advantages
- **Reliability**: Eliminates IP banning and bot detection issues completely
- **Maintenance**: Drastically reduced infrastructure complexity
- **Security**: No risk exposure from direct scraping
- **Performance**: Professional scraping infrastructure with built-in optimization

#### Strategic Benefits
- **Scalability**: Firecrawl infrastructure handles enterprise-scale scraping
- **Compliance**: Professional scraping service with proper rate limiting and ethics
- **Focus**: Team can focus 100% on AI extraction and data analysis features
- **Simplification**: Removes complex navigation logic and browser management

#### Technical Benefits
- **Bundle Size**: Remove Puppeteer and related dependencies
- **Code Complexity**: Eliminate PageNavigator and navigation caching logic
- **Error Handling**: Simpler error handling without fallback complexity
- **Configuration**: Cleaner configuration with fewer options

### Implementation Timeline

#### Week 1: Foundation
- Firecrawl service integration and API client
- Basic single-page scraping implementation
- Environment configuration and API key management
- Remove Puppeteer and PageNavigator dependencies

#### Week 2: Core Integration
- Update BasicScraper to use Firecrawl exclusively
- Schema validation integration with Firecrawl results
- Update configuration interfaces and test configs
- Remove legacy navigation code

#### Week 3: Multi-Page Support
- Firecrawl crawling integration with schema validation
- Batch processing and job status monitoring
- Performance monitoring and metrics collection
- Cost tracking and optimization

#### Week 4: Testing & Cleanup
- Comprehensive testing with updated configurations
- Legacy code removal (navigation caching, Puppeteer)
- Documentation updates and API reference
- Performance benchmarking vs previous implementation

### Risk Mitigation

#### Technical Risks
- **API Dependency**: Monitor Firecrawl service status and have alerting
- **Cost Overrun**: Implement usage monitoring and budget alerts
- **Rate Limiting**: Respect Firecrawl's rate limits and implement queuing

#### Business Risks
- **Service Availability**: Monitor uptime and have service level expectations
- **Pricing Changes**: Budget for Firecrawl costs as operational expense
- **API Changes**: Stay updated with Firecrawl API versioning and changelog

#### Migration Risks
- **Configuration Compatibility**: Ensure smooth transition of existing configs
- **Feature Parity**: Verify all current functionality works with Firecrawl
- **Performance**: Benchmark to ensure no significant performance regression
